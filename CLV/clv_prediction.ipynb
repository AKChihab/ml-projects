{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/19 01:06:35 INFO mlflow.tracking.fluent: Experiment with name 'CLV_Predictive_Project' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/akaoui/Desktop/Github/ml-projects/CLV/mlruns/757971774486516805', creation_time=1747609595118, experiment_id='757971774486516805', last_update_time=1747609595118, lifecycle_stage='active', name='CLV_Predictive_Project', tags={}>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configurer MLflow pour stockage local\n",
    "mlflow.set_tracking_uri(\"file://\" + os.path.abspath(\"./mlruns\"))\n",
    "mlflow.set_experiment(\"CLV_Predictive_Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock data: 1000 customers, 4609 transactions\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 1. MOCK DATA GENERATION\n",
    "# -------------------------------------------\n",
    "def generate_mock_data(n_customers=1000, start_date='2021-01-01', months=36, seed=42):\n",
    "    \"\"\"\n",
    "    1. Génère un DataFrame clients avec date de 1er achat aléatoire\n",
    "    2. Simule transactions (montant exponentiel + upsell binaire)\n",
    "    3. Ajoute mois de cohorte = période de first_purchase_date\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    customers = pd.DataFrame({'customer_id': range(1, n_customers+1)})\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end = start + pd.DateOffset(months=months)\n",
    "    customers['first_purchase_date'] = customers['customer_id'].apply(\n",
    "        lambda x: start + timedelta(days=np.random.randint(0, (end-start).days))\n",
    "    )\n",
    "    # Cohorte = mois (YYYY-MM) de 1er achat\n",
    "    customers['cohort_month'] = customers['first_purchase_date'].dt.to_period('M')\n",
    "\n",
    "    # Simuler transactions\n",
    "    transactions = []\n",
    "    for _, row in customers.iterrows():\n",
    "        cid, first = row['customer_id'], row['first_purchase_date']\n",
    "        n_tx = np.random.poisson(5)\n",
    "        for _ in range(n_tx):\n",
    "            tx_date = first + timedelta(days=np.random.exponential(scale=90))\n",
    "            if tx_date > end: continue\n",
    "            amount = np.random.exponential(scale=100)\n",
    "            upsell = np.random.binomial(1, 0.2)\n",
    "            transactions.append({'customer_id': cid, 'tx_date': tx_date,\n",
    "                                 'amount': amount, 'upsell': upsell})\n",
    "    tx_df = pd.DataFrame(transactions)\n",
    "    tx_df['tx_date'] = pd.to_datetime(tx_df['tx_date'])\n",
    "    return customers, tx_df\n",
    "\n",
    "# Exemple d'utilisation\n",
    "customers, transactions = generate_mock_data()\n",
    "#print(f\"Clients: {len(customers)}, Transactions: {len(transactions)}\")\n",
    "print(\"Mock data: {} customers, {} transactions\".format(\n",
    "    customers.shape[0], transactions.shape[0]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of      customer_id first_purchase_date cohort_month\n",
      "0              1          2023-05-11      2023-05\n",
      "1              2          2023-11-11      2023-11\n",
      "2              3          2021-05-02      2021-05\n",
      "3              4          2022-04-12      2022-04\n",
      "4              5          2021-11-27      2021-11\n",
      "..           ...                 ...          ...\n",
      "995          996          2021-05-08      2021-05\n",
      "996          997          2021-02-11      2021-02\n",
      "997          998          2021-02-10      2021-02\n",
      "998          999          2023-07-23      2023-07\n",
      "999         1000          2021-02-03      2021-02\n",
      "\n",
      "[1000 rows x 3 columns]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       customer_id                    tx_date      amount  upsell\n",
       "0               1 2023-05-24 05:58:09.957665  230.277153       1\n",
       "1               1 2023-07-31 21:15:49.356088   91.758372       0\n",
       "2               1 2023-05-28 08:29:44.094693  245.820956       0\n",
       "3               1 2023-06-23 11:31:09.575574   73.171693       0\n",
       "4               2 2023-12-06 08:02:24.261277   49.314226       0\n",
       "...           ...                        ...         ...     ...\n",
       "4604          999 2023-11-16 08:58:11.165026    5.053437       0\n",
       "4605         1000 2021-03-05 19:16:42.816585   36.583519       0\n",
       "4606         1000 2021-04-07 20:50:57.156885    1.177164       0\n",
       "4607         1000 2021-10-05 03:00:13.647492   37.183147       0\n",
       "4608         1000 2021-04-01 16:16:29.288050   82.483656       1\n",
       "\n",
       "[4609 rows x 4 columns]>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(customers.head)\n",
    "display(transactions.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of    cohort_month  n_customers\n",
      "0       2021-01           35\n",
      "1       2021-02           30\n",
      "2       2021-03           26\n",
      "3       2021-04           33\n",
      "4       2021-05           31\n",
      "5       2021-06           25\n",
      "6       2021-07           23\n",
      "7       2021-08           27\n",
      "8       2021-09           23\n",
      "9       2021-10           19\n",
      "10      2021-11           19\n",
      "11      2021-12           23\n",
      "12      2022-01           23\n",
      "13      2022-02           23\n",
      "14      2022-03           23\n",
      "15      2022-04           27\n",
      "16      2022-05           30\n",
      "17      2022-06           23\n",
      "18      2022-07           28\n",
      "19      2022-08           20\n",
      "20      2022-09           23\n",
      "21      2022-10           33\n",
      "22      2022-11           30\n",
      "23      2022-12           33\n",
      "24      2023-01           32\n",
      "25      2023-02           28\n",
      "26      2023-03           39\n",
      "27      2023-04           33\n",
      "28      2023-05           36\n",
      "29      2023-06           25\n",
      "30      2023-07           27\n",
      "31      2023-08           33\n",
      "32      2023-09           22\n",
      "33      2023-10           38\n",
      "34      2023-11           27\n",
      "35      2023-12           30>\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 2. COHORT SEGMENTATION\n",
    "# -------------------------------------------\n",
    "# Compute cohort metrics\n",
    "cohort_counts = customers.groupby('cohort_month').size().reset_index(name='n_customers')\n",
    "print(cohort_counts.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (972, 46)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 3. FEATURE ENGINEERING\n",
    "# -------------------------------------------\n",
    "def build_features(customers, transactions, observation_end):\n",
    "    \"\"\"\n",
    "    - Filtre transactions <= observation_end\n",
    "    - Calcule Recency (jours), Frequency (# tx), Monetary (somme)\n",
    "    - Normalise chaque métrique\n",
    "    - Compte upsell_count et propension = upsell_count/frequency\n",
    "    - One-hot encode cohort_month\n",
    "    \"\"\"\n",
    "    # Filter transactions until observation_end\n",
    "    tx = transactions[transactions['tx_date'] <= observation_end].copy()\n",
    "    # RFM features\n",
    "    agg = tx.groupby('customer_id').agg({\n",
    "        'tx_date': [lambda x: (observation_end - x.max()).days,\n",
    "                    lambda x: x.count()],\n",
    "        'amount': 'sum'\n",
    "    })\n",
    "    agg.columns = ['recency_days', 'frequency', 'monetary']\n",
    "    agg = agg.reset_index()\n",
    "    \n",
    "    # Normalize\n",
    "    for col in ['recency_days', 'frequency', 'monetary']:\n",
    "        agg[f'{col}_norm'] = (agg[col] - agg[col].mean()) / agg[col].std()\n",
    "    \n",
    "    # Merge cohort and upsell count\n",
    "    upsell_count = tx.groupby('customer_id')['upsell'].sum().reset_index(name='upsell_count')\n",
    "    agg = agg.merge(upsell_count, on='customer_id', how='left').fillna(0)\n",
    "\n",
    "    # Probability of upsell: upsell_count / frequency\n",
    "    agg['upsell_propensity'] = agg['upsell_count'] / agg['frequency']\n",
    "    agg['upsell_propensity'] = agg['upsell_propensity'].fillna(0)\n",
    "\n",
    "    # Merge cohort\n",
    "    agg = agg.merge(customers[['customer_id', 'cohort_month']], on='customer_id', how='left')\n",
    "    # One-hot encode cohorts\n",
    "    cohort_dummies = pd.get_dummies(agg['cohort_month'].astype(str), prefix='cohort')\n",
    "    agg = pd.concat([agg, cohort_dummies], axis=1)\n",
    "    \n",
    "    return agg\n",
    "\n",
    "# Build features as of a cutoff date\n",
    "observation_end = datetime(2023, 12, 31)\n",
    "features = build_features(customers, transactions, observation_end)\n",
    "print(\"Features shape:\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_id  future_clv  upsell_next_year\n",
      "0          175    35.18473                 0\n",
      "Target distribution: count     1.00000\n",
      "mean     35.18473\n",
      "std           NaN\n",
      "min      35.18473\n",
      "25%      35.18473\n",
      "50%      35.18473\n",
      "75%      35.18473\n",
      "max      35.18473\n",
      "Name: future_clv, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 4. TARGET DEFINITION: FUTURE CLV\n",
    "# -------------------------------------------\n",
    "\n",
    "def compute_targets(transactions, observation_end, horizon_days=365):\n",
    "    \"\"\"\n",
    "    - CLV: somme des montants entre (observation_end, observation_end+horizon)\n",
    "    - upsell_next_year: 1 si au moins un upsell durant cette période\n",
    "    \"\"\"\n",
    "    start = observation_end\n",
    "    end = observation_end + timedelta(days=horizon_days)\n",
    "    future = transactions[(transactions['tx_date']>start)&(transactions['tx_date']<=end)]\n",
    "    clv = future.groupby('customer_id')['amount'].sum().reset_index(name='future_clv')\n",
    "    ups = future.groupby('customer_id')['upsell'].max().reset_index(name='upsell_next_year')\n",
    "    return clv.merge(ups, on='customer_id', how='outer').fillna(0)\n",
    "\n",
    "targets = compute_targets(transactions, observation_end)\n",
    "print(targets.head())\n",
    "print(\"Target distribution:\", targets['future_clv'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [972, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clf\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Lancement des trainings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m clv_model = \u001b[43mtrain_clv_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m upsell_model = train_upsell_model(features, targets)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mtrain_clv_model\u001b[39m\u001b[34m(features, targets)\u001b[39m\n\u001b[32m      6\u001b[39m X = features.drop(columns=[\u001b[33m'\u001b[39m\u001b[33mcustomer_id\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      7\u001b[39m y = targets[\u001b[33m'\u001b[39m\u001b[33mfuture_clv\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m mlflow.start_run(run_name=\u001b[33m'\u001b[39m\u001b[33mtrain_clv\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     11\u001b[39m     rf = RandomForestRegressor(n_estimators=\u001b[32m100\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github/ml-projects/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github/ml-projects/.venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2848\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_arrays == \u001b[32m0\u001b[39m:\n\u001b[32m   2846\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAt least one array required as input\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2848\u001b[39m arrays = \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2850\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m   2851\u001b[39m n_train, n_test = _validate_shuffle_split(\n\u001b[32m   2852\u001b[39m     n_samples, test_size, train_size, default_test_size=\u001b[32m0.25\u001b[39m\n\u001b[32m   2853\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github/ml-projects/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:532\u001b[39m, in \u001b[36mindexable\u001b[39m\u001b[34m(*iterables)\u001b[39m\n\u001b[32m    502\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[32m    503\u001b[39m \n\u001b[32m    504\u001b[39m \u001b[33;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    528\u001b[39m \u001b[33;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    531\u001b[39m result = [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github/ml-projects/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:475\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    473\u001b[39m uniques = np.unique(lengths)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    476\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    477\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    478\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [972, 1]"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 5. MODEL TRAINING: PREDICTIVE CLV\n",
    "# -------------------------------------------\n",
    "\n",
    "def train_clv_model(features, targets):\n",
    "    X = features.drop(columns=['customer_id'])\n",
    "    y = targets['future_clv']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    with mlflow.start_run(run_name='train_clv'):\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        preds = rf.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, preds)\n",
    "        mlflow.log_metric('clv_mse', mse)\n",
    "        mlflow.sklearn.log_model(rf, 'model_clv')\n",
    "        print(f\"[CLV] MSE: {mse:.2f}\")\n",
    "    return rf\n",
    "# -------------------------------------------\n",
    "# 6. UPSALE PROPENSITY CLASSIFICATION\n",
    "# -------------------------------------------\n",
    "\n",
    "def train_upsell_model(features, targets):\n",
    "    X = features.drop(columns=['customer_id'])\n",
    "    y = targets['upsell_next_year']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    with mlflow.start_run(run_name='train_upsell'):\n",
    "        clf = LogisticRegression(max_iter=1000)\n",
    "        clf.fit(X_train, y_train)\n",
    "        probs = clf.predict_proba(X_test)[:,1]\n",
    "        auc = roc_auc_score(y_test, probs)\n",
    "        mlflow.log_metric('upsell_auc', auc)\n",
    "        mlflow.sklearn.log_model(clf, 'model_upsell')\n",
    "        print(f\"[Upsell] AUC: {auc:.2f}\")\n",
    "    return clf\n",
    "\n",
    "# Lancement des trainings\n",
    "clv_model = train_clv_model(features, targets)\n",
    "upsell_model = train_upsell_model(features, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 7. CONCLUSION ET SUITES POSSIBLES\n",
    "# -------------------------------------------\n",
    "# - Affiner le modèle avec hyperopt ou GridSearchCV\n",
    "# - Intégration MLflow\n",
    "# - Validation par retour terrain (expérimentations A/B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 5. MLOPS LOCAL SANS GCP\n",
    "# -------------------------------------------\n",
    "\n",
    "# 5.1 Containerisation : créer un Dockerfile\n",
    "# ------------------------------------------------\n",
    "# FROM python:3.9-slim\n",
    "# WORKDIR /app\n",
    "# COPY . /app\n",
    "# RUN pip install -r requirements.txt\n",
    "# ENTRYPOINT [\"python\", \"clv_predictive_mlops_project.py\"]\n",
    "\n",
    "# 5.2 Orchestration : exemple de DAG Airflow\n",
    "# ------------------------------------------------\n",
    "# Placez ce fichier dans AIRFLOW_HOME/dags/clv_dag.py\n",
    "# -------------------------------------------\n",
    "# from airflow import DAG\n",
    "# from airflow.operators.python import PythonOperator\n",
    "# from datetime import datetime, timedelta\n",
    "# \n",
    "# default_args = {'start_date': datetime(2023,1,1), 'retries': 1, 'retry_delay': timedelta(minutes=5)}\n",
    "# with DAG('clv_pipeline', schedule_interval='@monthly', default_args=default_args) as dag:\n",
    "#     task_gen = PythonOperator(task_id='generate_data', python_callable=generate_mock_data)\n",
    "#     task_feat = PythonOperator(task_id='build_features', python_callable=build_features,\n",
    "#                                 op_kwargs={'customers': customers, 'transactions': transactions, 'observation_end': observation_end})\n",
    "#     task_target = PythonOperator(task_id='compute_targets', python_callable=compute_targets,\n",
    "#                                 op_kwargs={'transactions': transactions, 'observation_end': observation_end})\n",
    "#     task_clv = PythonOperator(task_id='train_clv', python_callable=train_clv_model,\n",
    "#                               op_kwargs={'features': features, 'targets': targets})\n",
    "#     task_upsell = PythonOperator(task_id='train_upsell', python_callable=train_upsell_model,\n",
    "#                                  op_kwargs={'features': features, 'targets': targets})\n",
    "#     task_gen >> task_feat >> task_target >> [task_clv, task_upsell]\n",
    "\n",
    "# 5.3 Projet MLflow : MLproject (à placer racine)\n",
    "# ------------------------------------------------\n",
    "# name: CLV_Predictive_Project\n",
    "# conda_env: conda.yaml\n",
    "# entry_points:\n",
    "#   main:\n",
    "#     command: \"python clv_predictive_mlops_project.py\"\n",
    "\n",
    "# 5.4 Déploiement local de modèle via MLflow\n",
    "# ------------------------------------------------\n",
    "# mlflow models serve -m ./mlruns/0/xxx/artifacts/model_clv -p 1234\n",
    "\n",
    "# 5.5 Monitoring : Evidently pour drift detection\n",
    "# ------------------------------------------------\n",
    "# Installer: pip install evidently\n",
    "# Créer un dashboard dans un notebook ou service web\n",
    "\n",
    "# Fin du projet MLOps local"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
